/*! \page partitions How to partition large meshes in files with OpenAlya 
\section intro Introduction
This tutorial explains how to partition large meshes into large domain's number.It is usefull for meshes larger than 10 milion elements and 1000 domains.In Alya only the master is in charge to partition the mesh, so it's a process that needs a lot of memory for one single processor.For example, to partition a 43 million elements mesh in 50000 domains Alya needs about 32 GB of RAM.Most supercomputers don't have enough memory in order to partition that meshes, for example, MARENOSTRUM III only have 2GB per core.Because of that, we need to divide the process in two main parts:- Partition the mesh and store the results in small files, in a machine with a lot of memory- Start Alya simulation from the stored files in the supercomputer.The best machines in BSC to perform the partition preprocess step are Altix and Shiraz.Before doing this tasks we need to compile all the components: Metis5 and Alya with 8 Bytes integers.In summary, the steps to perform a simulation with large meshes are:-# Partion preprocess step:  -# Go to Altix or Shiraz machines.  -# Compile metis 5 and Alya with 8 Bytes integers  -# Execute Alya to generate partition mesh files.-# Execution case step:  -# Go to a supercomputer like MARENOSTRUM III  -# Copy the problem and the mesh files(generated in the last step) on the same folder  -# Compile metis 5 and Alya with 8 Bytes integers  -# Execute the case with AlyaHere we go with the details: \section compile_metis5 Compile metis 5 with 8 Bytes integersIf you are in marenostrum load the CMAKE tool with the command:module load CMAKE/2.8.12.2Go to "Thirdparties/metis-5.0.2_i8" and execute:  -#  make distclean  -#  make config  -#  makeThe librari libmetis.a is created in this folder:- build/Darwin-i386/libmetis for mac- build/Linux-x86_64/libmetis for linux\section compile_alya Compile Alya with 8 Bytes integers and metis 5We need to add this two directives to our Alya configure file:- -DI8: In order to compile with 8 Bytes integers. - -DV5METIS: In order to partition the mesh with Metis 5And also we need to link to the libmetis library compiled in the last step, the line that we need to add to our configure file is:- -L../../Thirdparties/metis-5.0.2_i8/build/Linux-x86_64/libmetis -lmetis (for linux)- -L../../Thirdparties/metis-5.0.2_i8/build/Darwin-i386/libmetis -lmetis (for mac)If your destination supercomputer is big endian and your partition machine is little endian you should put -DBIG_ENDIAN in all the line, at the end, for example:@codef90==    mpixlf90 -q64 -qflag=i -qnoescape -qmoddir=$O -qarch=qp -qtune=qp -qnostaticlink -I $O  -c -O3 -qsuffix=f=f90 -ig -DBIG_ENDIAN@endcodeHere are one example for ifort compiler: @code# start comment lines with a ##basic O2 optimizationf90==     mpif90 -O2 -DI8 -DV5METIS -m64 -c -module $O -tracebackfpp90==   mpif90 -O2 -DI8 -DV5METIS -m64 -c -module $O -traceback -fppfomp90==  mpif90 -O2 -DI8 -DV5METIS -m64 -c -module $O -traceback -fppcpp==     mpicc   -m64 -clink==    mpif90  -m64#for mac:#libs==    -L../../Thirdparties/metis-5.0.2_i8/build/Darwin-i386/libmetis -lmetis#for linux:libs==    -L../../Thirdparties/metis-5.0.2_i8/build/Linux-x86_64/libmetis -lmetisfa2p== mpif90 -m64 -c -C -ftrapuv -fpe0 -module ../../Utils/user/alya2pos -check none -traceback -debug full -warn all,nodec,nointerfaces -fp-stack-check -ansi-alias -I/usr/include -fpp @endcodeYo need to compile the PARALL service, the compilation line should be:./configure parall [list of required modules] -x -f=configure.in/your_configure_file\section Partion_preprocess_step Partion preprocess stepA machine with a minimum of 32GB per core is required for this step.The best machines to perform this action in BSC are Shiraz or the New Marenostrum big 256M nodes.-# Compile metis 5 and Alya following the instructions in:  - \ref compile_metis5  - \ref compile_alya-# Go to your case folder-# Create the folders needed to store the partitioned mesh files. Execute: Alya_folder/Utils/user/alya-hierarchy num_domains + 100  For example, to partition the mesh in 999 domains you have to execute Alya_folder/Utils/user/alya-hierarchy 1099,  This command creates folders called PARxxxxx, the folder PAR00000 stores the master information, the other folders stores  100 domain partition files data each one.-# Configure your corresponding case.dat file to tell to do the partition preprocess, add this lines:  @code    PARALL_SERVICE:         On             PARTITION_TYPE:       FACES             FILE_HIERARCHY:       ON             FILE_OPEN_CLOSE:      Yes             VIRTUAL_FILE:         On, MAXIMUM_MEMORY=0.5             TASK:                 ONLY_PREPROCESS, BINARY, SUBDOMAIN=num_domains    END_PARALL_SERVICE  @endcode  Substituting num_domains for the number of domains that you want, for example 999,  you have to take into acount that this number not includes the master process.-# Execute alya case without mpirun:\n  alya_folder/Executables/unix/Alya.x module_name\n-# Store your generated mesh partition files:  The folders PARxxxxx now contains yor mesh partitioned in the specified domains.  Tar this folders in one file.  \subsection Partition_preprocess_marenostrum Partition preprocess in Marenostrum nodesMarenostrum Nodes have been recently updated with more memory:- 64 nodes with 128 GB (upgraded nodes, HighMem)- 64 nodes with 64 GB (upgraded nodes, MedMem)- 2896 nodes with 32 GB (unchanged nodes, LowMem)In order to prepartition the mesh you can request a 64 or 128 GB node from marenostrum, you will need to specify the amount of memory per task you require. You can do that with the new parameter:#BSUB -M mem_requested_per_taskIn our case we will have only one task in the entire node, for ease of use follow this convention:* LowMem nodes:  #BSUB -M 30000* MedMem nodes:  #BSUB -M 60000* HighMem nodes: #BSUB -M 120000In order to obtain an entire node for our process we need to add these parameters to the job script:#BSUB -M 120000#BSUB -R "span[ptile=1]"#BSUB -xHere are an example of job prepartition script requesting a 128 GB node for Marenostrum:@code#!/bin/bash#BSUB -n 1#BSUB -M 120000#BSUB -R "span[ptile=1]"#BSUB -x#BSUB -oo output_%J.out#BSUB -eo output_%J.err#BSUB -J sequential#BSUB -W 04:00alya_folder/Executables/unix/Alya.x module_name@endcode\n\section Partion_postprocess_step Partion postprocess stepOnce you have partitioned your mesh you can run the simulation in the supercomputer:Marenostrum or Minotauro at BSC.Go to your supercomputer account and:-# Compile metis 5 and Alya following the instructions in:  - \ref compile_metis5  - \ref compile_alya-# Go to your case folder, that should have the same files as in the preprocess step, you can copy the problem from your original preprocess case folder.-# In the case folder extract or copy your PARxxxxx directories containing your partitioned mesh, obtained in the previous preprocess step.-# Configure your corresponding case.dat file to tell Alya to run the simulation with your prepartitioned mesh, add this lines:  @code    PARALL_SERVICE:         On             PARTITION_TYPE:       FACES             FILE_HIERARCHY:       ON             FILE_OPEN_CLOSE:      Yes             VIRTUAL_FILE:         On, MAXIMUM_MEMORY=0.5             TASK:                 READ_PREPROCESS, BINARY    END_PARALL_SERVICE  @endcode-# Execute alya case with mpirun:\n  mpirun -np num_domains + 1 alya_folder/Executables/unix/Alya.x module_name\n  For example: If your mesh is partitioned in 999 domains and cavtri03 module:\n  mpirun -np 1000 alya_folder/Executables/unix/Alya.x cavtri03\n  Example of marenostrum job to execute Alya:\n  @code  #!/bin/bash  #BSUB -n 1000  #BSUB -o output.out  #BSUB -e output.err  # In order to launch 128 processes with 16 processes per node:  #BSUB -R"span[ptile=16]"  #BSUB -J WRF.128-4  #BSUB -W 02:00  #You can choose the parallel environment through modules  module load intel openmpi  mpirun /alya_folder/Executables/unix/Alya.x fensap  @endcode-# After the simulation has  finished you can process the results with alya2pos utility as always\n
____
\n

*/
